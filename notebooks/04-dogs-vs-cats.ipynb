{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import collections\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "ROOT_DIR = os.getcwd() + '/..'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_cuda = False\n",
    "\n",
    "# paths\n",
    "data_path = ROOT_DIR + '/data/sample' # '/data'\n",
    "train_path = data_path + '/train/'\n",
    "valid_path = data_path + '/val/'\n",
    "# saved_model_path = ROOT_DIR + '/models/'\n",
    "# submission_path = ROOT_DIR + '/submissions/'\n",
    "\n",
    "# data\n",
    "batch_size = 8\n",
    "nb_train_samples = 20 # 25000\n",
    "nb_valid_samples = 10 # 2000\n",
    "# nb_test_samples  = 12500\n",
    "\n",
    "# model\n",
    "nb_runs = 1\n",
    "nb_aug = 3\n",
    "epochs = 2 #35\n",
    "lr = 1e-4\n",
    "clip = 0.001\n",
    "pin_memory = True if is_cuda else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(async=True) if is_cuda else target\n",
    "        image_var = torch.autograd.Variable(images)\n",
    "        label_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute y_pred\n",
    "        y_pred = model(image_var)\n",
    "        loss = criterion(y_pred, label_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec1 = accuracy(y_pred.data, target, topk=(1, 1))\n",
    "        losses.update(loss.data[0], images.size(0))\n",
    "        acc.update(prec1[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    acc = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, labels) in enumerate(val_loader):\n",
    "        labels = labels.cuda(async=True) if is_cuda else labels\n",
    "        image_var = torch.autograd.Variable(images, volatile=True)\n",
    "        label_var = torch.autograd.Variable(labels, volatile=True)\n",
    "\n",
    "        # compute y_pred\n",
    "        y_pred = model(image_var)\n",
    "        loss = criterion(y_pred, label_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, temp_var = accuracy(y_pred.data, labels, topk=(1, 1))\n",
    "        losses.update(loss.data[0], images.size(0))\n",
    "        acc.update(prec1[0], images.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    print('   * EPOCH {epoch} | Accuracy: {acc.avg:.3f} | Loss: {losses.avg:.3f}'.format(epoch=epoch,\n",
    "                                                                                         acc=acc,\n",
    "                                                                                         losses=losses))\n",
    "\n",
    "    return acc.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    global lr\n",
    "    lr = lr * (0.1**(epoch // 30))\n",
    "    for param_group in optimizer.state_dict()['param_groups']:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(y_pred, y_actual, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = y_actual.size(0)\n",
    "\n",
    "    _, pred = y_pred.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(y_actual.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.vgg16(pretrained=True)\n",
    "model = models.resnet152(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Linear(25088, 4096),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.Dropout(p=0.5),\n",
    "#     nn.Linear(4096, 4096),\n",
    "#     nn.ReLU(inplace=True),\n",
    "#     nn.Dropout(p=0.5),\n",
    "#     nn.Linear(4096, 2)\n",
    "# )\n",
    "model.fc = nn.Linear(2048, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.features = torch.nn.DataParallel(model.features)\n",
    "if is_cuda:\n",
    "    model.cuda()\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir = train_path\n",
    "valdir = valid_path\n",
    "# testdir = test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(\n",
    "    datasets.ImageFolder(traindir, transforms.Compose([\n",
    "         transforms.RandomSizedCrop(224),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.ToTensor(),\n",
    "         normalize])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=pin_memory)\n",
    "\n",
    "val_loader = data.DataLoader(\n",
    "    datasets.ImageFolder(valdir, transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=pin_memory)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda() if is_cuda else nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.classifier.parameters(), lr, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   * EPOCH 0 | Accuracy: 100.000 | Loss: 0.375\n",
      "   * EPOCH 1 | Accuracy: 100.000 | Loss: 0.373\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    # train for one epoch\n",
    "    train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    prec1 = validate(val_loader, model, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
